
# -*- coding: utf-8 -*-
"""ANN1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yXoYGra2HMKapVv1eQXYfHBBHkf4NZwV
    link for data set: https://www.kaggle.com/datasets/shrutimechlearn/churn-modelling
"""

!pip install tensorflow-gpu

import tensorflow as tf
print(tf.__version__)

#importing some basic libraries
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

dataset=pd.read_csv('Churn_Modelling.csv')

dataset.head()

#Dividing data into independent and dependent features
x=dataset.iloc[:,3:13]
y=dataset.iloc[:,13]

x.head()

y.head()

## Feature Engineering
##We are using one hot encoding so the other categories can be found easily 
##drop_first is used to drop one so the rest is self explanatory
geography = pd.get_dummies(x['Geography'],drop_first=True)
gender=pd.get_dummies(x['Gender'],drop_first=True)

## concatinate the above variables with the dataframe we need to drop the existing category
x=x.drop(['Geography','Gender'],axis=1)

x.head()

## concatinate the above variables with the dataframe
x=pd.concat([x,geography,gender],axis=1)

#Splitting the dataset into training set and test set
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

#feature scaling 
## Why standardscaler? minmax scaler is for CNN ,Standardscaler is based on Z score if data varies from mean to standard deviation then we use standard scaler
##We use fit_transform() on the train data so that we learn the parameters of scaling on the train data and in the same time we scale the train data.
## We only use transform() on the test data because we use the scaling paramaters learned on the train data to scale the test data.

from sklearn.preprocessing import StandardScaler 
sc = StandardScaler()
x_train=sc.fit_transform(x_train)
x_test=sc.transform(x_test)

x_train

x_test

x_train.shape

##Lets create a ANN
##
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LeakyReLU,PReLU,ELU,ReLU
from tensorflow.keras.layers import Dropout

##Initialization of ANN
classifier= Sequential()

## Adding the input layer
## Number of inputs can be observed from the shape of x_train that is 11
##Dense to add layers
classifier.add(Dense(units=11,activation='relu'))

##Adding the first hidden layer
classifier.add(Dense(units=7,activation='relu'))

##Adding the second hidden layer
classifier.add(Dense(units=6,activation='relu'))

##Adding the output layer
classifier.add(Dense(units=1,activation='sigmoid'))

## Training of nueral newtwork
##by default adam optimizer has learning rate
classifier.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])

##Specifying a specific learning rate
import tensorflow
opt=tensorflow.keras.optimizers.Adam(learning_rate=0.01)

##Early stopping it used to stop the model the when there is no much is change in accuracy
import tensorflow as tf

early_stopping=tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0.0001,
    patience=20,
    verbose=1,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
)

##Training of nueral newtwork
##Since the epoch value is high accuracy will be same at some point this leads to what should be the number of epochs?
##Validation split is we are taking that data to split for training
model_history=classifier.fit(x_train,y_train,validation_split=0.33,batch_size=10,epochs=1000,callbacks=early_stopping)

model_history.history.keys()

#Summarize history for accuracy
plt.plot(model_history.history['accuracy'])
plt.plot(model_history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'],loc = 'upper left')
plt.show()

#Summarize history for loss
plt.plot(model_history.history['loss'])
plt.plot(model_history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train','test'],loc = 'upper left')
plt.show()

##Prediction for test data
y_pred = classifier.predict(x_test)
y_pred = (y_pred>=0.5)

##Confusion matrix
from sklearn.metrics import confusion_matrix
cm=confusion_matrix(y_test,y_pred)
cm

## calculate accuracy
from sklearn.metrics import accuracy_score
score = accuracy_score(y_pred,y_test)
score

## get the weights
classifier.get_weights()
